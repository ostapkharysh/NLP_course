{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package treebank to\n",
      "[nltk_data]     /home/ostapkharysh/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/treebank.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function bigrams at 0x7f6b84309268>\n",
      "<function trigrams at 0x7f6b843092f0>\n",
      "<function ngrams at 0x7f6b843091e0>\n"
     ]
    }
   ],
   "source": [
    "print(nltk.bigrams)\n",
    "print(nltk.trigrams)\n",
    "print(nltk.ngrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text Analytics with Python.\n",
    "A Practical Real-World Approach to Gaining Actionable Insights from your Data\n",
    "Dipanjan Sarkar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid character in identifier (<ipython-input-30-8b292661e5a5>, line 8)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-30-8b292661e5a5>\"\u001b[0;36m, line \u001b[0;32m8\u001b[0m\n\u001b[0;31m    №norm_alice = filter(None, normalize_corpus(alice, lemmatize=False))\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid character in identifier\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import gutenberg\n",
    "#from normalization import normalize_corpus\n",
    "import nltk\n",
    "from operator import itemgetter\n",
    "# load corpus\n",
    "alice = gutenberg.sents(fileids='carroll-alice.txt')\n",
    "alice = [' '.join(ts) for ts in alice]\n",
    "№norm_alice = filter(None, normalize_corpus(alice, lemmatize=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "Missing parentheses in call to 'print' (<ipython-input-31-2a7b28da4f16>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-31-2a7b28da4f16>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    print norm_alice[0]\u001b[0m\n\u001b[0m                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m Missing parentheses in call to 'print'\n"
     ]
    }
   ],
   "source": [
    "# print first line\n",
    "print norm_alice[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_corpus(corpus):\n",
    "    return ' '.join([document.strip()for document in corpus])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ngrams(sequence, n):\n",
    "    return zip(*[sequence[index:] for index in range(n)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_ngrams([1,2,3,4], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_ngrams([1,2,3,4], 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_ngrams(corpus, ngram_val=1, limit=5):\n",
    "    corpus = flatten_corpus(corpus)\n",
    "    tokens = nltk.word_tokenize(corpus)\n",
    "    ngrams = compute_ngrams(tokens, ngram_val)\n",
    "    ngrams_freq_dist = nltk.FreqDist(ngrams)\n",
    "    sorted_ngrams_fd = sorted(ngrams_freq_dist.items(),     key=itemgetter(1), reverse=True)\n",
    "    sorted_ngrams = sorted_ngrams_fd[0:limit]\n",
    "    sorted_ngrams = [(' '.join(text), freq) for text, freq in sorted_ngrams]\n",
    "    return sorted_ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top 10 bigrams\n",
    "get_top_ngrams(corpus=norm_alice, ngram_val=2, limit=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_top_ngrams(corpus=norm_alice, ngram_val=3, limit=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://www.katrinerk.com/courses/python-worksheets/language-models-in-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am Sam </s> <s> I do not like green eggs and ham </s> <s> Sam </s> <s> I do not like green eggs and ham </s> <s> I am Sam I am </s> <s> Sam </s> <s> I am </s> <s> Sam </s> <s> I do not like green \n",
      "\n",
      "a thousand Siddo . Gradually , as you have '' ? `` Ekstrohm said , but they would join the skiff hove into a nestling , and instrumental . The half-man lay back . Any little hand is a creepy feeling to see stars in lightyears or future , it *** Introductory Examples for the NLTK Book ***\n",
      "Loading text1, ..., text9 and sent1, ..., sent9\n",
      "Type the name of the text or sentence to view it.\n",
      "Type: 'texts()' or 'sents()' to list the materials.\n",
      "text1: Moby Dick by Herman Melville 1851\n",
      "text2: Sense and Sensibility by Jane Austen 1811\n",
      "text3: The Book of Genesis\n",
      "text4: Inaugural Address Corpus\n",
      "text5: Chat Corpus\n",
      "text6: Monty Python and the Holy Grail\n",
      "text7: Wall Street Journal\n",
      "text8: Personals Corpus\n",
      "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'initialword' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-1682772d9e71>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m \u001b[0mword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minitialword\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'initialword' is not defined"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "from nltk.corpus import brown\n",
    "\n",
    "# an nltk.FreqDist() is like a dictionary,\n",
    "# but it is ordered by frequency.\n",
    "# Also, nltk automatically fills the dictionary\n",
    "# with counts when given a list of words.\n",
    "\n",
    "freq_brown = nltk.FreqDist(brown.words())\n",
    "\n",
    "list(freq_brown.keys())[:20]\n",
    "freq_brown.most_common(20)\n",
    "\n",
    "# an nltk.ConditionalFreqDist() counts frequencies of pairs.\n",
    "# When given a list of bigrams, it maps each first word of a bigram\n",
    "# to a FreqDist over the second words of the bigram.\n",
    "\n",
    "cfreq_brown_2gram = nltk.ConditionalFreqDist(nltk.bigrams(brown.words()))\n",
    "\n",
    "# conditions() in a ConditionalFreqDist are like keys()\n",
    "# in a dictionary\n",
    "\n",
    "cfreq_brown_2gram.conditions()\n",
    "\n",
    "# the cfreq_brown_2gram entry for \"my\" is a FreqDist.\n",
    "\n",
    "cfreq_brown_2gram[\"my\"]\n",
    "\n",
    "# here are the words that can follow after \"my\".\n",
    "# We first access the FreqDist associated with \"my\",\n",
    "# then the keys in that FreqDist\n",
    "\n",
    "cfreq_brown_2gram[\"my\"].keys()\n",
    "\n",
    "# here are the 20 most frequent words to come after \"my\", with their frequencies\n",
    "\n",
    "cfreq_brown_2gram[\"my\"].most_common(20)\n",
    "\n",
    "# an nltk.ConditionalProbDist() maps pairs to probabilities.\n",
    "# One way in which we can do this is by using Maximum Likelihood Estimation (MLE)\n",
    "\n",
    "cprob_brown_2gram = nltk.ConditionalProbDist(cfreq_brown_2gram, nltk.MLEProbDist)\n",
    "\n",
    "# This again has conditions() wihch are like dictionary keys\n",
    "\n",
    "cprob_brown_2gram.conditions()\n",
    "\n",
    "# Here is what we find for \"my\": a Maximum Likelihood Estimation-based probability distribution,\n",
    "# as a MLEProbDist object.\n",
    "\n",
    "cprob_brown_2gram[\"my\"]\n",
    "\n",
    "# We can find the words that can come after \"my\" by using the function samples()\n",
    "\n",
    "cprob_brown_2gram[\"my\"].samples()\n",
    "\n",
    "# Here is the probability of a particular pair:\n",
    "\n",
    "\n",
    "\n",
    "cprob_brown_2gram[\"my\"].prob(\"own\")\n",
    "\n",
    "#####\n",
    "\n",
    "# We can also compute unigram probabilities (probabilities of individual words)\n",
    "\n",
    "freq_brown_1gram = nltk.FreqDist(brown.words())\n",
    "\n",
    "len_brown = len(brown.words())\n",
    "\n",
    "\n",
    "def unigram_prob(word):\n",
    "\n",
    "    return freq_brown_1gram[ word] / len_brown\n",
    "\n",
    "\n",
    "\n",
    "#############\n",
    "\n",
    "# The contents of cprob_brown_2gram, all these probabilities, now form a\n",
    "\n",
    "# trained bigram language model. The typical use for a language model is\n",
    "\n",
    "# to ask it for the probabillity of a word sequence\n",
    "\n",
    "# P(how do you do) = P(how) * P(do|how) * P(you|do) * P(do | you)\n",
    "\n",
    "prob_sentence = unigram_prob(\"how\") * cprob_brown_2gram[\"how\"].prob(\"do\") * cprob_brown_2gram[\"do\"].prob(\"you\") * cprob_brown_2gram[\"you\"].prob(\"do\")\n",
    "\n",
    "# result: 1.5639033871961e-09\n",
    "\n",
    "\n",
    "###############\n",
    "\n",
    "# We can also use a language model in another way:\n",
    "\n",
    "# We can let it generate text at random\n",
    "\n",
    "# This is not so useful, but can be insightful into what it is that\n",
    "\n",
    "# the language model has been learning\n",
    "\n",
    "\n",
    "\n",
    "cprob_brown_2gram[\"my\"].generate()\n",
    "\n",
    "# We can use this to generate text at random\n",
    "# based on a given text of bigrams.\n",
    "# Let's do this for the Sam \"corpus\"\n",
    "\n",
    "corpus = \"\"\"<s> I am Sam </s>\n",
    "<s> Sam I am </s>\n",
    "<s> I do not like green eggs and ham </s>\"\"\"\n",
    "\n",
    "words = corpus.split()\n",
    "cfreq_sam = nltk.ConditionalFreqDist(nltk.bigrams(words))\n",
    "cprob_sam = nltk.ConditionalProbDist(cfreq_sam, nltk.MLEProbDist)\n",
    "\n",
    "word = \"<s>\"\n",
    "for index in range(50):\n",
    "    word = cprob_sam[ word].generate()\n",
    "    print(word, end = \" \")\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "# Not a lot of variety. We need a bigger corpus.\n",
    "# What kind of genres do we have in the Brown corpus?\n",
    "brown.categories()\n",
    "\n",
    "# Let's try Science Fiction.\n",
    "cfreq_scifi = nltk.ConditionalFreqDist(nltk.bigrams(brown.words(categories = \"science_fiction\")))\n",
    "cprob_scifi = nltk.ConditionalProbDist(cfreq_scifi, nltk.MLEProbDist)\n",
    "\n",
    "word = \"in\"\n",
    "for index in range(50):\n",
    "    word = cprob_scifi[ word ].generate()\n",
    "    print(word, end = \" \")\n",
    "print()\n",
    "\n",
    "# try this with other Brown corpus categories.\n",
    "\n",
    "\n",
    "# Here is how to do this with NLTK books:\n",
    "import nltk\n",
    "from nltk.book import *\n",
    "\n",
    "def generate_text(text, initialword, numwords):\n",
    "    bigrams = list(nltk.ngrams(text, 2))\n",
    "    cpd = nltk.ConditionalProbDist(nltk.ConditionalFreqDist(bigrams), nltk.MLEProbDist)\n",
    "\n",
    "\n",
    "word = initialword\n",
    "for i in range(numwords):\n",
    "    print(word, end = \" \")\n",
    "    word = cpd[ word].generate()\n",
    "    print(word)\n",
    "\n",
    "# Holy Grail\n",
    "generate_text(text6, \"I\", 100)\n",
    "# sense and sensibility\n",
    "generate_text(text2, \"I\", 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-34-4201941e78f9>, line 83)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-34-4201941e78f9>\"\u001b[0;36m, line \u001b[0;32m83\u001b[0m\n\u001b[0;31m    for bigram, count in sorted_bigrams\u001b[0m\n\u001b[0m                                       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Katrin Erk Oct 07\n",
    "# Updated Feb 11\n",
    "#\n",
    "# Word bigrams are just pairs of words.\n",
    "# In the sentence \"I went to the beach\"\n",
    "# the bigrams are:\n",
    "#    I went\n",
    "#    went to\n",
    "#    to the\n",
    "#    the beach\n",
    "#\n",
    "# Having counts of English bigrams from a very large text corpus\n",
    "# can be useful for a number of purposes.\n",
    "#\n",
    "# for example for spelling correction:\n",
    "# If I had mistyped the sentence as \"I went to beach\"\n",
    "# then I might be able to find the error by seeing that\n",
    "# the bigram \"to beach\" has a very low count, and\n",
    "# \"to the\", \"to a\", and \"the beach\" have much larger counts.\n",
    "#\n",
    "# This program counts all word bigrams in a given text file\n",
    "#\n",
    "# usage:\n",
    "# python3 count_bigrams.py <filename>\n",
    "#\n",
    "# <filename> is a text file.\n",
    "\n",
    "import string\n",
    "import sys\n",
    "\n",
    "# complain if we didn't get a filename\n",
    "# as a command line argument\n",
    "if len(sys.argv) < 2:\n",
    "    print(\"Please enter the name of a corpus file as a command line argument.\")\n",
    "    sys.exit()\n",
    "   \n",
    "# try opening file\n",
    "# If the file doesn't exist, catch the error\n",
    "try:\n",
    "    f = open(sys.argv[1])\n",
    "except IOError:\n",
    "    print(\"Sorry, I could not find the file\", sys.argv[1])\n",
    "    print(\"Please try again.\")\n",
    "    sys.exit()\n",
    "   \n",
    "# read the contents of the whole file into ''filecontents''\n",
    "filecontents = f.read()\n",
    "       \n",
    "# count bigrams\n",
    "bigrams = {}\n",
    "words_punct = filecontents.split()\n",
    "# strip all punctuation at the beginning and end of words, and\n",
    "# convert all words to lowercase.\n",
    "# The following is a Python list comprehension. It is a command that transforms a list,\n",
    "# here words_punct, into another list.\n",
    "words = [ w.strip(string.punctuation).lower() for w in words_punct ]\n",
    "\n",
    "# add special START, END tokens\n",
    "words = [\"START\"] + words + [\"END\"]\n",
    "\n",
    "for index, word in enumerate(words):\n",
    "    if index < len(words) - 1:\n",
    "        # we only look at indices up to the\n",
    "        # next-to-last word, as this is\n",
    "        # the last one at which a bigram starts\n",
    "        w1 = words[index]\n",
    "        w2 = words[index + 1]\n",
    "        # bigram is a tuple,\n",
    "        # like a list, but fixed.\n",
    "        # Tuples can be keys in a dictionary\n",
    "        bigram = (w1, w2)\n",
    "\n",
    "        if bigram in bigrams:\n",
    "            bigrams[ bigram ] = bigrams[ bigram ] + 1\n",
    "        else:\n",
    "            bigrams[ bigram ] = 1\n",
    "        # or, more simply, like this:\n",
    "        # bigrams[bigram] = bigrams.get(bigram, 0) + 1\n",
    "\n",
    "# sort bigrams by their counts\n",
    "sorted_bigrams = sorted(bigrams.items(), key = lambda pair:pair[1], reverse = True)\n",
    "\n",
    "for bigram, count in sorted_bigrams\n",
    "    print(bigram, \":\", count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bi-gram extraction and Language model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://appliedmachinelearning.blog/2017/04/30/language-identification-from-texts-using-bi-gram-model-pythonnltk/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.collocations import BigramCollocationFinder\n",
    "import re\n",
    "import codecs\n",
    "import numpy as np\n",
    "import string\n",
    "\n",
    "def train_language(path,lang_name):\n",
    "    words_all = []\n",
    "    translate_table = dict((ord(char), None) for char in string.punctuation)\n",
    "    # reading the file in unicode format using codecs library    \n",
    "    with codecs.open(path,\"r\",\"utf-8\") as filep:\n",
    "         \n",
    "        for i,line in enumerate(filep):            \n",
    "            # extracting the text sentence from each line         \n",
    "            line = \" \".join(line.split()[1:])\n",
    "            line = line.lower()   # to lower case\n",
    "            line = re.sub(r\"\\d+\", \"\", line) # remove digits\n",
    "            \n",
    "            if len(line) != 0:\n",
    "                line = line.translate(translate_table) # remove punctuations\n",
    "                words_all += line\n",
    "                words_all.append(\" \") # append sentences with space\n",
    "               \n",
    "    all_str = ''.join(words_all)\n",
    "    all_str = re.sub(' +',' ',all_str) # replace series of spaces with single space\n",
    "    seq_all = [i for i in all_str]\n",
    "    \n",
    "    # extracting the bi-grams and sorting them according to their frequencies\n",
    "    finder = BigramCollocationFinder.from_words(seq_all)\n",
    "    finder.apply_freq_filter(5)\n",
    "    bigram_model = finder.ngram_fd.viewitems()\n",
    "    bigram_model = sorted(finder.ngram_fd.viewitems(), key=lambda item: item[1],reverse=True)  \n",
    "    \n",
    "    print bigram_model   \n",
    "    np.save(lang_name+\".npy\",bigram_model) # save language model\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    root = \"train\\\\\"\n",
    "    lang_name = [\"french\",\"english\",\"german\",\"italian\",\"dutch\",\"spanish\"]\n",
    "    train_lang_path = [\"fra_news_2010_30K-text\\\\fra_news_2010_30K-sentences.txt\",\"eng_news_2015_30K\\\\eng_news_2015_30K-sentences.txt\",\"deu_news_2015_30K\\\\deu_news_2015_30K-sentences.txt\",\"ita_news_2010_30K-text\\\\ita_news_2010_30K-sentences.txt\",\"nld_wikipedia_2016_30K\\\\nld_wikipedia_2016_30K-sentences.txt\",\"spa_news_2011_30K\\\\spa_news_2011_30K-sentences.txt\"]\n",
    "    for i,p in enumerate(train_lang_path):\n",
    "        train_language(root+p,lang_name[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python/gensim: Creating bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://markhneedham.com/blog/2015/02/12/pythongensim-creating-bigrams-over-how-i-met-your-mother-transcripts/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import csv\n",
    "import string\n",
    "from gensim.models import Phrases\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "sentences = []\n",
    "bigram = Phrases()\n",
    "with open(\"data/import/sentences.csv\", \"r\") as sentencesfile:\n",
    "    reader = csv.reader(sentencesfile, delimiter = \",\")\n",
    "    reader.next()\n",
    "    for row in reader:\n",
    "        sentence = [word.decode(\"utf-8\")\n",
    "                    for word in nltk.word_tokenize(row[4].lower())\n",
    "                    if word not in string.punctuation]\n",
    "        sentences.append(sentence)\n",
    "        bigram.add_vocab([sentence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(bigram[sentences])[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_counter = Counter()\n",
    "for key in bigram.vocab.keys():\n",
    "    if key not in stopwords.words(\"english\"):\n",
    "        if len(key.split(\"_\")) > 1:\n",
    "            bigram_counter[key] += bigram.vocab[key]\n",
    "\n",
    "for key, counts in bigram_counter.most_common(20):\n",
    "    print '{0: <20} {1}'.format(key.encode(\"utf-8\"), counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Kneser Ney smoo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://stackoverflow.com/questions/35242155/kneser-ney-smoothing-of-trigrams-using-python-nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import ngrams\n",
    "    from nltk.corpus import gutenberg\n",
    "\n",
    "    gut_ngrams = ( ngram for sent in gutenberg.sents() for ngram in ngrams(sent, 3, pad_left = True, pad_right = True, right_pad_symbol='EOS', left_pad_symbol=\"BOS\"))\n",
    "    freq_dist = nltk.FreqDist(gut_ngrams)\n",
    "    kneser_ney = nltk.KneserNeyProbDist(freq_dist)\n",
    "\n",
    "    prob_sum = 0\n",
    "    for i in kneser_ney.samples():\n",
    "        if i[0] == \"I\" and i[1] == \"confess\":\n",
    "            prob_sum += kneser_ney.prob(i)\n",
    "            print \"{0}:{1}\".format(i, kneser_ney.prob(i))\n",
    "    print prob_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Завдання 1. Впорядкувати, перевірити notepad.\n",
    "Завдання 2. Вивчити засоби бібліотеки gensim по створенню n-gram моделей мови https://radimrehurek.com/gensim/models/phrases.html \n",
    "Завдання 3. N-грам моделі для української мови. Література, моделі, власні моделі (Вікіпедія)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
